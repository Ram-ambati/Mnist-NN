# ğŸ§  Neural Network from Scratch

[![Python](https://img.shields.io/badge/Python-3.7%2B-blue?style=flat-square&logo=python)](https://www.python.org/)
[![NumPy](https://img.shields.io/badge/NumPy-v1.20%2B-orange?style=flat-square&logo=numpy)](https://numpy.org/)

A **pure NumPy** implementation of a fully connected neural network trained on MNIST digit classification. Built from first principles with no deep learning frameworks â€” just linear algebra and calculus.

## âœ¨ Features

- âœ… Custom Dense layer with forward and backward propagation
- âœ… ReLU and Softmax activation functions
- âœ… Mini-batch stochastic gradient descent (SGD)
- âœ… Full backpropagation algorithm implementation
- âœ… Training and inference on MNIST dataset
- âœ… Detailed performance visualization

## ğŸ“ Project Structure

| File | Purpose |
|------|---------|
| `network.py` | Main neural network architecture with forward/backward passes |
| `layer.py` | Dense layer with weight/bias parameters and gradient computation |
| `activations.py` | ReLU and Softmax activation functions and derivatives |
| `data_loader.py` | MNIST data loading, preprocessing, and one-hot encoding |
| `train.py` | Training loop with mini-batch SGD and model serialization |
| `test.py` | Model evaluation, accuracy metrics, and visualizations |

## ğŸ›  Requirements

```
numpy>=1.20.0
pandas>=1.2.0
matplotlib>=3.3.0
seaborn>=0.11.0
scikit-learn>=0.24.0
```

## ğŸ“¦ Installation

```bash
# Clone the repository
git clone <https://github.com/Ram-ambati/Mnist-NN.git>
cd "Mnist-NN"

# Install dependencies
pip install -r requirements.txt
```

Or install manually:
```bash
pip install numpy pandas matplotlib seaborn scikit-learn
```

## ğŸš€ Quick Start

### Training the Model

```bash
python train.py
```

**Output:**
```
Epoch 0 accuracy 0.8742
Epoch 1 accuracy 0.9154
Epoch 2 accuracy 0.9281
...
Epoch 9 accuracy 0.9634
```

The trained model is saved as `model.pkl` for later inference.

### Testing & Visualization

```bash
python test.py
```

Generates:
- **Accuracy Report** - Overall test accuracy
- **Misclassification Grid** - 5Ã—5 grid of wrongly classified samples with predictions
- **Confusion Matrix Heatmap** - Detailed per-digit performance analysis

## ğŸ— Network Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Input Layer    â”‚  784 neurons (28Ã—28 pixels)
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚   Dense Layer   â”‚  784 â†’ 128 neurons
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  ReLU Activationâ”‚  Max(0, x)
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚   Dense Layer   â”‚  128 â†’ 10 neurons
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Softmax Output  â”‚  10-way classification
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Network Details

| Layer | Input Size | Output Size | Activation | Parameters |
|-------|-----------|------------|-----------|-----------|
| Dense 1 | 784 | 128 | ReLU | 100,480 |
| Dense 2 | 128 | 10 | Softmax | 1,290 |
| **Total** | - | - | - | **101,770** |

## âš™ Hyperparameters

| Parameter | Value | Description |
|-----------|-------|------------|
| Learning Rate | 0.01 | Step size for weight updates |
| Epochs | 10 | Number of training passes |
| Batch Size | 32 | Samples per gradient update |
| Optimizer | SGD | Stochastic Gradient Descent |
| Weight Init | N(0, 0.01) | Small random initialization |

## ğŸ“Š Dataset

**MNIST Handwritten Digits**
- **Format:** CSV (label + 784 pixel values)
- **Training Set:** 60,000 samples
- **Test Set:** 10,000 samples
- **Image Size:** 28Ã—28 pixels (grayscale)
- **Pixel Range:** 0-255 (normalized to 0-1)
- **Classes:** 10 digits (0-9)

```
Sample row: [5, 0, 0, 18, 127, 234, ..., 45, 12, 0]
            â””â”€ label â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ pixel values â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ’¡ How It Works

### Forward Pass
1. Input: 784-dimensional vector
2. Linear transformation: `zâ‚ = Wx + b`
3. Activation: `aâ‚ = ReLU(zâ‚)`
4. Output layer: `zâ‚‚ = Waâ‚ + b`
5. Probabilities: `p = Softmax(zâ‚‚)`

### Backward Pass
1. Compute loss gradient: `dL/dzâ‚‚ = (predictions - targets)`
2. Layer 2 gradients: `dW, db`
3. Backprop activation: `daâ‚ = ReLU'(zâ‚) âŠ™ dzâ‚‚áµ€W`
4. Layer 1 gradients: `dW, db`
5. Update weights: `W â† W - lrÂ·dW`

## ğŸ“ˆ Performance

Typical results after 10 epochs:
- **Test Accuracy:** ~93%
- **Training Time:** ~2-3 minutes (CPU)
- **Inference Time:** ~50ms per 10k samples


## ğŸ“ License

MIT License - feel free to use this for learning and projects!
